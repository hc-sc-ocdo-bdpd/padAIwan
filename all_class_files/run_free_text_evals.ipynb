{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77db1422",
   "metadata": {},
   "source": [
    "# üß™ Free-text Field Evaluation Runner\n",
    "\n",
    "This notebook runs LLM-based evaluation for free-text fields on all model outputs, saving the results to Excel files (`free_text_eval_{model}_{split}.xlsx`) for downstream analysis and reporting.\n",
    "\n",
    "- **Inputs:** All model prediction folders under `outputs/`\n",
    "- **Outputs:** One Excel file per model and split with LLM-scored results\n",
    "- **No performance aggregation or summary in this notebook.**  \n",
    "  See main evaluation notebook for metrics and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b74a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9f576",
   "metadata": {},
   "source": [
    "## 1. Setup Azure OpenAI Client\n",
    "\n",
    "Environment variables required:\n",
    "- `ENDPOINT_URL`\n",
    "- `AZURE_OPENAI_API_KEY`\n",
    "- `DEPLOYMENT_NAME`\n",
    "- `AZURE_OPENAI_API_VERSION` (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1110c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure settings\n",
    "load_dotenv()\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"ENDPOINT_URL\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\"),\n",
    ")\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "# Regex to strip code fences from LLM output\n",
    "fence_re = re.compile(r\"^```(?:json)?\\s*|\\s*```$\", flags=re.MULTILINE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb84a8",
   "metadata": {},
   "source": [
    "## 2. Run LLM Evaluation for Each Model and Split\n",
    "\n",
    "This cell will:\n",
    "- Scan all model subfolders in `outputs/`\n",
    "- For each model and split (`train`, `test`), run LLM evaluation for all Included records\n",
    "- Save results to `free_text_eval_{model}_{split}.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b906499",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_root = Path.cwd() / \"outputs\"\n",
    "ft_metrics = []\n",
    "\n",
    "for model_dir in outputs_root.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    model = model_dir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = model_dir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "        preds = sorted(preds_dir.glob(\"*.json\"))\n",
    "        n_skipped = 0\n",
    "\n",
    "        for jf in tqdm(preds, desc=f\"eval free-text {model} {split}\"):\n",
    "            rec = json.loads(jf.read_text(encoding=\"utf-8\"))\n",
    "            gt  = rec[\"ground_truth\"]\n",
    "            pr  = rec[\"prediction\"]\n",
    "\n",
    "            # Require both truth and prediction to be 'Included'\n",
    "            cls_pred = pr.get(\"classification\", \"\")\n",
    "            cls_truth = gt.get(\"classification\", \"\")\n",
    "            if not (isinstance(cls_pred, str) and cls_pred.strip().lower() == \"included\"):\n",
    "                n_skipped += 1\n",
    "                continue\n",
    "            if not (isinstance(cls_truth, str) and cls_truth.strip().lower() == \"included\"):\n",
    "                n_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Extract ground-truth free-text fields\n",
    "            aud_gt = gt.get(\"audience\", \"\") or \"\"\n",
    "            meth_gt = gt.get(\"methodology\", \"\") or \"\"\n",
    "            sz_raw = gt.get(\"sample_size\", None)\n",
    "            sz_gt = str(sz_raw).strip() if sz_raw not in (None, \"\") else \"\"\n",
    "            idfac_gt = gt.get(\"identity_factors\") or gt.get(\"identity_factors_gt\") or \"\"\n",
    "\n",
    "            # Skip if all truth fields are blank\n",
    "            if not any([aud_gt.strip(), meth_gt.strip(), sz_gt, idfac_gt.strip()]):\n",
    "                n_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Extract predicted free-text fields\n",
    "            aud_pr = pr.get(\"audience\", \"\") or \"\"\n",
    "            meth_pr = pr.get(\"methodology\", \"\") or \"\"\n",
    "            sz_pr = str(pr.get(\"sample_size\", \"\")).strip()\n",
    "            idfac_pr = pr.get(\"Identity_Factors\", \"\") or \"\"\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "You are an evaluator of free-text fields. Compare these pairs:\n",
    "\n",
    "Ground truth Audience: {aud_gt}\n",
    "Predicted Audience: {aud_pr}\n",
    "\n",
    "Ground truth Methodology: {meth_gt}\n",
    "Predicted Methodology: {meth_pr}\n",
    "\n",
    "Ground truth Sample Size: {sz_gt}\n",
    "Predicted Sample Size: {sz_pr}\n",
    "\n",
    "Ground truth Identity Factors: {idfac_gt}\n",
    "Predicted Identity Factors: {idfac_pr}\n",
    "\n",
    "Be lenient and forgiving:\n",
    "- Audience is correct if the core demographic appears in the predicted text even with extra qualifiers\n",
    "- Methodology is correct if it‚Äôs semantically equivalent or more specific\n",
    "- Sample Size is correct if numeric values match ignoring formatting (for example \"n=20\" vs \"20\")\n",
    "- Identity Factors is correct if core identity considerations in ground-truth appear in the prediction even with extra qualifiers\n",
    "\n",
    "Reply in JSON format like this:\n",
    "\n",
    "{{\n",
    "  \"audience_correct\": 0_or_1,\n",
    "  \"methodology_correct\": 0_or_1,\n",
    "  \"sample_size_correct\": 0_or_1,\n",
    "  \"identity_factors_correct\": 0_or_1\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "            comp = {\n",
    "                \"audience_correct\": None,\n",
    "                \"methodology_correct\": None,\n",
    "                \"sample_size_correct\": None,\n",
    "                \"identity_factors_correct\": None\n",
    "            }\n",
    "            for _ in range(5):\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=DEPLOYMENT_NAME,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful assistant that evaluates the similarity of pairs of free-text fields and responds in JSON format.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0\n",
    "                )\n",
    "                text = fence_re.sub(\"\", resp.choices[0].message.content.strip())\n",
    "                try:\n",
    "                    comp = json.loads(text)\n",
    "                    break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "            ft_metrics.append({\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"id\": jf.stem,\n",
    "                \"audience_gt\": aud_gt,\n",
    "                \"audience_pred\": aud_pr,\n",
    "                \"audience_correct\": comp.get(\"audience_correct\", None),\n",
    "                \"methodology_gt\": meth_gt,\n",
    "                \"methodology_pred\": meth_pr,\n",
    "                \"methodology_correct\": comp.get(\"methodology_correct\", None),\n",
    "                \"sample_size_gt\": sz_gt,\n",
    "                \"sample_size_pred\": sz_pr,\n",
    "                \"sample_size_correct\": comp.get(\"sample_size_correct\", None),\n",
    "                \"identity_factors_gt\": idfac_gt,\n",
    "                \"identity_factors_pred\": idfac_pr,\n",
    "                \"identity_factors_correct\": comp.get(\"identity_factors_correct\", None),\n",
    "            })\n",
    "\n",
    "        # Save XLSX for this model and split\n",
    "        df_split = pd.DataFrame([r for r in ft_metrics if r[\"model\"] == model and r[\"split\"] == split])\n",
    "        if not df_split.empty:\n",
    "            out_dir = outputs_root / model / split\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = out_dir / f\"free_text_eval_{model}_{split}.xlsx\"\n",
    "            df_split[[\n",
    "                \"id\",\n",
    "                \"audience_gt\", \"audience_pred\", \"audience_correct\",\n",
    "                \"methodology_gt\", \"methodology_pred\", \"methodology_correct\",\n",
    "                \"sample_size_gt\", \"sample_size_pred\", \"sample_size_correct\",\n",
    "                \"identity_factors_gt\", \"identity_factors_pred\", \"identity_factors_correct\"\n",
    "            ]].to_excel(file_path, index=False)\n",
    "            print(f\"Saved {file_path}\")\n",
    "        else:\n",
    "            print(f\"Nothing to score for {model} {split}. Skipped {n_skipped} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fab141",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úîÔ∏è All Done!\n",
    "\n",
    "Your free-text evaluations have been saved as Excel files in each model's `outputs/{model}/{split}/` directory.\n",
    "\n",
    "**You can now return to your main evaluation notebook to aggregate and visualize results.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
