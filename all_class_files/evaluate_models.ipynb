{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de73e4a",
   "metadata": {},
   "source": [
    "# ğŸ“ŠğŸ§ª Literature Screening â€“ Multiâ€‘class Model Evaluation\n",
    "Aggregate results for **train** and **test** splits, report core metrics for the binary *Included vs Excluded* task, then evaluate the extra fields returned *only* for items predicted as **Included**.\n",
    "\n",
    "We focus on the structured fields that are straightforward to score:\n",
    "* **domain**Â â†’Â matches column **Social, Behavioural or Implementation Science?**\n",
    "* **dmf_stage**Â â†’Â matches column **DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties**\n",
    "* **decision_type**Â â†’Â matches column **DMF - Are the decisions regulatory, policy, or other? Please describe the â€œotherâ€ if applicable.**  \n",
    "  For **decision_type** a prediction that *contains* the word **other** is considered correct when the groundâ€‘truth field also contains **other** (ignoring any freeâ€‘text description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 1 â€“ Imports and helpers ğŸ”Œ                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2 â€“ Locate outputs and datasets ğŸ”         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "notebook_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "root_dir      = notebook_dir  # all_class_files/\n",
    "outputs_root  = root_dir / \"outputs\"\n",
    "datasets_dir  = root_dir.parent / \"datasets\"\n",
    "\n",
    "if not outputs_root.exists():\n",
    "    raise RuntimeError(f\"Could not find outputs directory at: {outputs_root}\")\n",
    "if not datasets_dir.exists():\n",
    "    raise RuntimeError(f\"Could not find datasets directory at: {datasets_dir}\")\n",
    "\n",
    "# every subâ€‘folder inside outputs/ is a model name\n",
    "model_dirs = [d for d in outputs_root.iterdir() if d.is_dir() and d.name != \"datasets\"]\n",
    "if not model_dirs:\n",
    "    raise RuntimeError(f\"No model result folders found inside '{outputs_root}/'\")\n",
    "\n",
    "print(\"Models found:\", \", \".join(d.name for d in model_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2.5 â€“ Load and clean ground-truth data ğŸ—„ï¸ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def load_ground_truth(split: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and cleans ground-truth data for a given split.\n",
    "    Returns a DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    if split not in [\"train\", \"test\"]:\n",
    "        raise ValueError(f\"Invalid split: {split}. Must be either 'train' or 'test'\")\n",
    "    \n",
    "    # load ground-truth data\n",
    "    csv_path = datasets_dir / f\"{split}_dataset.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    df_t = pd.read_csv(csv_path)\n",
    "    truth_cols = {\n",
    "        \"id\": \"id\",\n",
    "        \"label\": \"ground_truth\",\n",
    "        \"Social, Behavioural or Implementation Science?\": \"domain_gt\",\n",
    "        \"DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties\": \"dmf_stage_gt\",\n",
    "        \"DMF - Are the decisions regulatory, policy, or other? Please describe the â€œotherâ€ if applicable.\": \"decision_type_gt\",\n",
    "        \"IS - Does your submission include or intersect with Black, Indigenous or racialized groups?\": \"bipoc_gt\",\n",
    "        \"IS - Does your submission include or intersect with Indigenous Peoples?\": \"indigenous_gt\",\n",
    "        \"IS - Have you included Sex in your study:\": \"sex_gt\",\n",
    "        \"IS - Have you included Gender in your study?\": \"gender_gt\",\n",
    "    }\n",
    "    df_t = df_t.rename(columns=truth_cols)[list(truth_cols.values())]\n",
    "\n",
    "    # clean ground-truth data\n",
    "\n",
    "    for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "        df_t[col] = df_t[col].apply(lambda x: x.strip().lower() if isinstance(x, str) else \"\")\n",
    "\n",
    "    allowed_labels = [\"included\", \"excluded\"]\n",
    "    df_t.loc[~df_t[\"ground_truth\"].isin(allowed_labels), \"ground_truth\"] = None\n",
    "\n",
    "    allowed_domains = [\"social\", \"behavioural\", \"implementation\"]\n",
    "    df_t.loc[~df_t[\"domain_gt\"].isin(allowed_domains), \"domain_gt\"] = None\n",
    "        \n",
    "    allowed_stages = [\"identify the issue and its context\", \"assess risks and benefits\", \"identify and analyze options\", \"select a strategy\", \"implement the strategy\", \"monitor and evaluate results\", \"involve interested and affected parties\"]\n",
    "    df_t.loc[~df_t[\"dmf_stage_gt\"].isin(allowed_stages), \"dmf_stage_gt\"] = None\n",
    "\n",
    "    allowed_decision_types = [\"regulatory\", \"policy\", \"other\", \"not stated\"]        \n",
    "    df_t.loc[df_t[\"decision_type_gt\"].str.contains(\"other\"), \"decision_type_gt\"] = \"other\"\n",
    "    df_t.loc[~df_t[\"decision_type_gt\"].isin(allowed_decision_types), \"decision_type_gt\"] = None\n",
    "\n",
    "    allowed_identity = [\"yes\", \"not reported\"]       \n",
    "    for col in [\"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "        df_t.loc[df_t[col].str.contains(\"yes\"), col] = \"yes\"\n",
    "        df_t.loc[~df_t[col].isin(allowed_identity), col] = None\n",
    "\n",
    "    return df_t\n",
    "\n",
    "# Load ground-truth data for both splits\n",
    "df_train = load_ground_truth(\"train\")\n",
    "df_test = load_ground_truth(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2.7 â€“ Explore ground-truth data ğŸ—„ï¸ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Look at the distribution of ground-truth labels in the training set.\n",
    "print(f\"There are {len(df_train)} rows in the training set.\")\n",
    "for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "    counts = df_train[col].value_counts(dropna=False)\n",
    "    counts_norm = df_train[col].value_counts(normalize=True) * 100\n",
    "    None_count = counts.get(None, 0)\n",
    "    print(f\"\\nThere are {None_count} None elements in the {col} column.\")\n",
    "    print(f\"If we ignore None elements, the labels in {col} are distributed as follows:\")\n",
    "    print(counts_norm.rename_axis(None).rename(None).map(lambda x: f\"{x:.1f}%\").to_string())\n",
    "\n",
    "# Look at the distribution of ground-truth labels in the test set.\n",
    "print(f\"\\n\\nThere are {len(df_test)} rows in the test set.\")\n",
    "for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "    counts = df_test[col].value_counts(dropna=False)\n",
    "    counts_norm = df_test[col].value_counts(normalize=True) * 100\n",
    "    None_count = counts.get(None, 0)\n",
    "    print(f\"\\nThere are {None_count} None elements in the {col} column.\")\n",
    "    print(f\"If we ignore None elements, the labels in {col} are distributed as follows:\")\n",
    "    print(counts_norm.rename_axis(None).rename(None).map(lambda x: f\"{x:.1f}%\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06682c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2.8 â€“ Display ground-truth label distribution ğŸ—„ï¸ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import math\n",
    "\n",
    "cols = [\"ground_truth\", \"domain_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]\n",
    "num_fig_cols = math.ceil(len(cols)/2)\n",
    "fig, ax = plt.subplots(2, num_fig_cols, figsize=(20,10))\n",
    "for i, col in enumerate(cols):\n",
    "    counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "    counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "    df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "    df_counts.columns = ['train', 'test']\n",
    "    df_counts.rename_axis(None, inplace=True)\n",
    "    axis = ax[i // num_fig_cols, i % num_fig_cols]\n",
    "    df_counts.plot(kind='bar', ax=axis, rot=0)\n",
    "    axis.set_title(col)\n",
    "    axis.set_ylabel('%')\n",
    "    axis.legend(loc='upper right')\n",
    "for j in range(i+1, 2*num_fig_cols):\n",
    "    fig.delaxes(ax[j // num_fig_cols, j % num_fig_cols])  # remove empty subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "col = \"dmf_stage_gt\"  # plot label distribution separately for \"dmf_stage_gt\"        \n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(20, 5)) \n",
    "counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "df_counts.columns = ['train', 'test']\n",
    "df_counts.rename_axis(None, inplace=True)\n",
    "axis = ax2\n",
    "df_counts.plot(kind='bar', ax=axis, rot=45)\n",
    "axis.set_title(col)\n",
    "axis.set_ylabel('%')\n",
    "axis.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2.9 â€“ Create figure for report - Label distributions                   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8.2))\n",
    "gs = fig.add_gridspec(3, 4)\n",
    "\n",
    "cols = [\"ground_truth\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]\n",
    "col_titles = ['Label', 'BIPOC', 'Indigenous', 'Sex', 'Gender']\n",
    "for i, col in enumerate(cols):\n",
    "    counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "    counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "    df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "    df_counts.columns = ['train', 'test']\n",
    "    df_counts.rename_axis(None, inplace=True)\n",
    "    if i == 0:\n",
    "        row = 0\n",
    "        col_idx = 0\n",
    "    else:\n",
    "        row = 2\n",
    "        col_idx = i - 1 \n",
    "    axis = fig.add_subplot(gs[row, col_idx])\n",
    "    df_counts.plot(kind='bar', ax=axis, rot=0, width=0.35)\n",
    "    axis.set_title(col_titles[i])\n",
    "    axis.set_ylabel('%')\n",
    "    axis.get_legend().set_visible(False)\n",
    "\n",
    "cols = [\"domain_gt\", \"decision_type_gt\"]\n",
    "col_titles = ['Domain', 'Decision type']\n",
    "for i, col in enumerate(cols):\n",
    "    counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "    counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "    df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "    df_counts.columns = ['train', 'test']\n",
    "    df_counts.rename_axis(None, inplace=True)\n",
    "    if i == 0:\n",
    "        axis = fig.add_subplot(gs[1, 0:2])\n",
    "        df_counts.plot(kind='bar', ax=axis, rot=0, width=0.22)\n",
    "    else:    \n",
    "        axis = fig.add_subplot(gs[1, 2:4])\n",
    "        df_counts.plot(kind='bar', ax=axis, rot=0, width=0.30)\n",
    "    axis.set_title(col_titles[i])\n",
    "    axis.set_ylabel('%')\n",
    "    axis.get_legend().set_visible(False)\n",
    "\n",
    "col = \"dmf_stage_gt\"      \n",
    "counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "df_counts.columns = ['train', 'test']\n",
    "df_counts.rename_axis(None, inplace=True)\n",
    "axis = fig.add_subplot(gs[0, 1:4])\n",
    "df_counts.plot(kind='bar', ax=axis, rot=0, width=0.33)\n",
    "axis.set_title(\"Dmf stage\")\n",
    "axis.set_xticklabels([\n",
    "    \"assess risks\\nand benefits\",\n",
    "    \"identify and\\nanalyze\\noptions\",\n",
    "    \"monitor and\\nevaluate\\nresults\",\n",
    "    \"identify the\\nissue and\\nits context\",\n",
    "    \"implement\\nthe strategy\",\n",
    "    \"involve\\ninterested and\\naffected parties\",\n",
    "    \"select a\\nstrategy\"\n",
    "])\n",
    "axis.set_ylabel('%')\n",
    "axis.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle(\"Label Distributions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 3 â€“ Load predictions and merge ground-truth ğŸ—„ï¸ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Structure: {model: {split: DataFrame}}\n",
    "all_predictions = defaultdict(dict)\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    model_name = mdir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = mdir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for jf in preds_dir.glob(\"*.json\"):\n",
    "            with open(jf, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            pred_block = data.get(\"prediction\", {})\n",
    "            x = {\n",
    "                \"id\": jf.stem,\n",
    "                \"pred_class\": pred_block.get(\"classification\"),\n",
    "                \"domain_pred\": pred_block.get(\"domain\"),\n",
    "                \"dmf_stage_pred\": pred_block.get(\"dmf_stage\"),\n",
    "                \"decision_type_pred\": pred_block.get(\"decision_type\"),\n",
    "                \"bipoc_pred\": pred_block.get(\"BIPOC\"),\n",
    "                \"indigenous_pred\": pred_block.get(\"Indigenous\"),\n",
    "                \"sex_pred\": pred_block.get(\"Sex\"),\n",
    "                \"gender_pred\": pred_block.get(\"Gender\"),\n",
    "            }\n",
    "            rows.append(x)\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        df_pred = pd.DataFrame(rows)\n",
    "\n",
    "        # Clean prediction data\n",
    "        for col in [\"pred_class\", \"domain_pred\", \"dmf_stage_pred\", \"decision_type_pred\", \"bipoc_pred\", \"indigenous_pred\", \"sex_pred\", \"gender_pred\"]:\n",
    "            df_pred[col] = df_pred[col].apply(lambda x: str(x).strip().lower() if x is not None else None)\n",
    "\n",
    "        if split == \"train\":\n",
    "            df_truth = df_train\n",
    "        else:\n",
    "            df_truth = df_test\n",
    "\n",
    "        df = pd.merge(df_pred, df_truth, on=\"id\", how=\"left\")\n",
    "        all_predictions[model_name][split] = df\n",
    "\n",
    "        unparsable = (df[\"pred_class\"] == \"parseerror\").sum()\n",
    "        print(f\"{model_name} [{split}] -> {len(df):,} rows, {unparsable} unparsable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 4 â€“ Core binaryâ€‘classification metrics ğŸ“‹ â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "metrics_cls = []\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"included\", \"excluded\"])]\n",
    "        unparsed = len(df) - len(parsable)\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        metrics_cls.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"n_total\": len(df),\n",
    "                \"n_unparsed\": unparsed,\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred, pos_label=\"included\"),\n",
    "                \"recall\": recall_score(y_true, y_pred, pos_label=\"included\"),\n",
    "                \"f1\": f1_score(y_true, y_pred, pos_label=\"included\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "metrics_cls_df = (\n",
    "    pd.DataFrame(metrics_cls)\n",
    "    .set_index([\"model\", \"split\"])\n",
    "    .sort_values([\"model\", \"split\"])\n",
    ")\n",
    "metrics_cls_df.style.format({\"accuracy\": \"{:.3f}\", \"precision\": \"{:.3f}\", \"recall\": \"{:.3f}\", \"f1\": \"{:.3f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f849f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 5 â€“ Visualise binary metrics ğŸ“Š           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "for split in (\"train\", \"test\"):\n",
    "    subset = metrics_cls_df.xs(split, level=\"split\")\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    subset[\"accuracy\"].plot(kind=\"bar\", ax=axes[0])\n",
    "    axes[0].set_title(f\"Accuracy ({split})\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    subset[\"f1\"].plot(kind=\"bar\", ax=axes[1])\n",
    "    axes[1].set_title(f\"F1â€‘score ({split})\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    (subset[\"n_unparsed\"] / subset[\"n_total\"]).plot(kind=\"bar\", ax=axes[2])\n",
    "    axes[2].set_title(f\"Unparsed % ({split})\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(f\"Model comparison on {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 5.5 â€“ Create figure for report - Performance measures - 'Label' field  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if not isinstance(metrics_cls_df.index, pd.RangeIndex):\n",
    "    metrics_cls_df = metrics_cls_df.reset_index()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "i = 0\n",
    "for col in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n",
    "    metrics_cls_pivot = (\n",
    "        metrics_cls_df\n",
    "        .pivot(index=\"model\", columns=\"split\", values=col)\n",
    "        .reindex([\"gpt-4.1\", \"gpt-4o\", \"o3\"])\n",
    "        .reindex(columns=[\"train\", \"test\"])\n",
    "    )\n",
    "    axis = axes[i]\n",
    "    metrics_cls_pivot.plot(kind=\"bar\", ax=axis, rot=0)\n",
    "    axis.set_title(f\"{col.capitalize()}\")\n",
    "    axis.set_xlabel(\"\")\n",
    "    axis.set_xticklabels(['GPT-4.1', 'GPT-4o', 'o3'])\n",
    "    axis.set_ylim(0, 1)\n",
    "    if col == \"recall\":\n",
    "        axis.legend(title=\"\")\n",
    "    else:\n",
    "        axis.get_legend().set_visible(False) \n",
    "    i += 1\n",
    "\n",
    "fig.suptitle(\"Performance Measures - 'Label' Field\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 6 â€“ Extra-field scoring ğŸ·ï¸               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "extra_metrics = []\n",
    "fields = [\n",
    "    (\"domain_pred\",       \"domain_gt\",       \"domain\"),\n",
    "    (\"dmf_stage_pred\",    \"dmf_stage_gt\",    \"dmf_stage\"),\n",
    "    (\"decision_type_pred\",\"decision_type_gt\",\"decision_type\"),\n",
    "    (\"bipoc_pred\",        \"bipoc_gt\",        \"bipoc\"),\n",
    "    (\"indigenous_pred\",   \"indigenous_gt\",   \"indigenous\"),\n",
    "    (\"sex_pred\",          \"sex_gt\",          \"sex\"),\n",
    "    (\"gender_pred\",       \"gender_gt\",       \"gender\")\n",
    "]\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "\n",
    "        # evaluate only items predicted as \"included\"  \n",
    "        include_rows = df[\"pred_class\"] == \"included\"\n",
    "\n",
    "        # do not evaluate items for which ground truth label is 'included' but other ground truth fields are None\n",
    "        if fields:\n",
    "            conditions = [df[\"ground_truth\"] == \"included\"]\n",
    "            for field in fields:\n",
    "                conditions.append(df[field[1]].isna())\n",
    "            exclude_rows = pd.concat(conditions, axis=1).all(axis=1)\n",
    "            df_inc = df[include_rows & ~exclude_rows].copy()\n",
    "        else:\n",
    "            df_inc = df[include_rows].copy()\n",
    "\n",
    "        if df_inc.empty:\n",
    "            continue\n",
    "\n",
    "        for pcol, tcol, name in fields:\n",
    "            \n",
    "            # exclude rows where ground_truth label is \"included\" and value in tcol equals None\n",
    "            excl = (df_inc[\"ground_truth\"] == \"included\") & df_inc[tcol].isna()\n",
    "\n",
    "            if (~excl).sum() > 0:\n",
    "                acc = accuracy_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''))\n",
    "                cl = df_inc.loc[~excl, tcol].fillna('').unique()\n",
    "                class_labels = [x for x in cl if x]  # remove empty string from class labels                        \n",
    "                f1 = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average='macro', labels=class_labels)\n",
    "                #f1_weighted = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average='weighted', labels=class_labels)\n",
    "                #f1_none = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average=None, labels=class_labels)\n",
    "                #print(f\"{model} {split} {name} f1_macro: {f1:.3f} f1_weighted: {f1_weighted:.3f}\")\n",
    "                #print(f\"{class_labels} : {f1_none}\")                \n",
    "            else:\n",
    "                acc = float(\"nan\")\n",
    "                f1 = float(\"nan\")\n",
    "\n",
    "            extra_metrics.append(\n",
    "                {\n",
    "                    \"model\"   : model,\n",
    "                    \"split\"   : split,\n",
    "                    \"field\"   : name,\n",
    "                    \"n_scored\": (~excl).sum(),\n",
    "                    \"accuracy\": round(acc, 3),\n",
    "                    \"f1\"      : round(f1, 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# ---- pivot & pretty-print ---- #\n",
    "a_extra = pd.DataFrame(extra_metrics)\n",
    "if not a_extra.empty:\n",
    "    pivot_acc = a_extra.pivot(index=[\"model\", \"split\"], columns=\"field\", values=\"accuracy\")\n",
    "    pivot_acc = pivot_acc[[\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]]\n",
    "    print('Accuracy:')\n",
    "    display(pivot_acc.style.format(\"{:.3f}\"))\n",
    "    pivot_f1 = a_extra.pivot(index=[\"model\", \"split\"], columns=\"field\", values=\"f1\")\n",
    "    pivot_f1 = pivot_f1[[\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]]\n",
    "    print('F1â€‘score:')\n",
    "    display(pivot_f1.style.format(\"{:.3f}\"))\n",
    "else:\n",
    "    print(\"Cannot score extra fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 7 â€“ Visualise extra-field accuracy and f1 score ğŸ“ŠğŸ·ï¸   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "if not a_extra.empty:\n",
    "    for field in [\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]:\n",
    "        fig, ax = plt.subplots(figsize=(9, 4))\n",
    "        subset_acc = a_extra[a_extra[\"field\"] == field].pivot(\n",
    "            index=\"model\", columns=\"split\", values=\"accuracy\"\n",
    "        )\n",
    "        subset_acc.plot(kind=\"bar\", ax=ax, rot=0)  # one bar-group per model\n",
    "        ax.set_title(f\"{field} â€“ accuracy by model and split\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"model\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(9, 4))\n",
    "        subset_f1 = a_extra[a_extra[\"field\"] == field].pivot(\n",
    "            index=\"model\", columns=\"split\", values=\"f1\"\n",
    "        )\n",
    "        subset_f1.plot(kind=\"bar\", ax=ax2, rot=0)  # one bar-group per model\n",
    "        ax2.set_title(f\"{field} â€“ f1 score by model and split\")\n",
    "        ax2.set_ylabel(\"f1 score\")\n",
    "        ax2.set_xlabel(\"model\")\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 8 â€“ Per-model feature accuracy and f1 score ğŸ“Š         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "if not a_extra.empty:\n",
    "    for model_name in a_extra[\"model\"].unique():\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        subset_acc = (\n",
    "            a_extra[a_extra[\"model\"] == model_name]\n",
    "            .pivot(index=\"field\", columns=\"split\", values=\"accuracy\")\n",
    "            .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "        )\n",
    "        subset_acc.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_title(f\"{model_name} â€“ extra-field accuracy\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"field\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "        subset_f1 = (\n",
    "            a_extra[a_extra[\"model\"] == model_name]\n",
    "            .pivot(index=\"field\", columns=\"split\", values=\"f1\")\n",
    "            .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "        )\n",
    "        subset_f1.plot(kind=\"bar\", ax=ax2, rot=0)\n",
    "        ax2.set_title(f\"{model_name} â€“ extra-field f1 score\")\n",
    "        ax2.set_ylabel(\"f1 score\")\n",
    "        ax2.set_xlabel(\"field\")\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a596b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 8.5 â€“ Create figure for report - extra field performance measures ğŸ”² â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if not a_extra.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    subset_acc = (\n",
    "        a_extra[a_extra[\"split\"] == 'test']\n",
    "        .pivot(index=\"field\", columns=\"model\", values=\"accuracy\")\n",
    "        .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "    )\n",
    "    subset_acc.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "    ax.set_title(f\"Test Set Accuracy\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(\"Field\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticklabels([\"domain\", \"dmf stage\", \"decision type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "    ax.legend([\"GPT-4.1\", \"GPT-4o\", \"o3\"], title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "    subset_f1 = (\n",
    "        a_extra[a_extra[\"split\"] == \"test\"]\n",
    "        .pivot(index=\"field\", columns=\"model\", values=\"f1\")\n",
    "        .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "    )\n",
    "    subset_f1.plot(kind=\"bar\", ax=ax2, rot=0)\n",
    "    ax2.set_title(f\"Test Set F1 Scores\")\n",
    "    ax2.set_ylabel(\"F1 score\")\n",
    "    ax2.set_xlabel(\"Field\")\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_xticklabels([\"domain\", \"dmf stage\", \"decision type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "    ax2.legend([\"GPT-4.1\", \"GPT-4o\", \"o3\"], title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0173cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 9 â€“ Confusion matrices for binary task ğŸ”² â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"included\", \"excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"included\", \"excluded\"])\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Purples\",\n",
    "            xticklabels=[\"Included\", \"Excluded\"],\n",
    "            yticklabels=[\"Included\", \"Excluded\"],\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix â€“ {model} ({split})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Classification report for {model} ({split})\")\n",
    "        print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 9.5 â€“ Create figure for report - confusion matrices for include/exclude field ğŸ”² â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(8, 5))\n",
    "row = 0\n",
    "col = 0\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"included\", \"excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"included\", \"excluded\"])\n",
    "        axis = ax[row, col]\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Purples\",\n",
    "            xticklabels=[\"Included\", \"Excluded\"],\n",
    "            yticklabels=[\"Included\", \"Excluded\"],\n",
    "            ax=axis,\n",
    "            vmin=0,\n",
    "            vmax=250 if split == \"train\" else 65,\n",
    "        )\n",
    "        \n",
    "        if model == 'gpt-4.1':\n",
    "            axis.set_title(f\"GPT-4.1 - {split}\")\n",
    "        elif model == 'gpt-4o':\n",
    "            axis.set_title(f\"GPT-4o - {split}\")\n",
    "        else:\n",
    "            axis.set_title(f\"o3 - {split}\")\n",
    "\n",
    "        if row == 1:\n",
    "            axis.set_xlabel(\"Predicted\")\n",
    "        if col == 0:\n",
    "            axis.set_ylabel(\"Actual\")\n",
    "\n",
    "        row += 1\n",
    "    col += 1   \n",
    "    row = 0 \n",
    "fig.suptitle(\"Confusion Matrices - 'Label' Field\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 10 - Freeâ€‘text field evaluation summary from XLSX results â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Note: This evaluation occurs after binary classification is complete. The results are only represent performance on true positives (include/exclude column).\n",
    "\n",
    "outputs_root = Path.cwd() / \"outputs\"\n",
    "ft_metrics = []\n",
    "\n",
    "for model_dir in outputs_root.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    model = model_dir.name\n",
    "    for split in (\"train\", \"test\"):\n",
    "        file_path = model_dir / split / f\"free_text_eval_{model}_{split}.xlsx\"\n",
    "        if file_path.exists():\n",
    "            df = pd.read_excel(file_path)\n",
    "            df[\"model\"] = model\n",
    "            df[\"split\"] = split\n",
    "            ft_metrics.append(df)\n",
    "        else:\n",
    "            print(f\"Warning: Missing file {file_path}\")\n",
    "\n",
    "if ft_metrics:\n",
    "    df_ft = pd.concat(ft_metrics, ignore_index=True)\n",
    "    summary = (\n",
    "        df_ft\n",
    "        .melt(\n",
    "            id_vars=[\"model\",\"split\",\"id\"],\n",
    "            value_vars=[\"audience_correct\",\"methodology_correct\",\"sample_size_correct\",\"identity_factors_correct\"],\n",
    "            var_name=\"field\",\n",
    "            value_name=\"correct\"\n",
    "        )\n",
    "        .groupby([\"model\",\"split\",\"field\"])\n",
    "        .agg(n_scored=(\"correct\",\"size\"), accuracy=(\"correct\",\"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    display(\n",
    "        summary\n",
    "        .pivot(index=[\"model\",\"split\"], columns=\"field\", values=\"accuracy\")\n",
    "        .style.format(\"{:.3f}\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No evaluation xlsx files found. Run free-text inferences first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 10.5 â€“ Create figure for report - free-text field performance measures ğŸ”² â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if not summary.empty:\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    subset = (\n",
    "        summary[summary[\"split\"] == 'test']\n",
    "        .pivot(index=\"field\", columns=\"model\", values=\"accuracy\")\n",
    "        .reindex([\"audience_correct\", \"methodology_correct\", \"sample_size_correct\", \"identity_factors_correct\"])\n",
    "    )\n",
    "    subset.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "    ax.set_title(\"Test Set Accuracy\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(\"Field\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticklabels([\"audience\", \"methodology\", \"sample size\", \"identity factors\"])\n",
    "    ax.legend([\"GPT-4.1\", \"GPT-4o\", \"o3\"], title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084af19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
