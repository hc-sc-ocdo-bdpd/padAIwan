{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de73e4a",
   "metadata": {},
   "source": [
    "# 📊🧪 Literature Screening – Multi‑class Model Evaluation\n",
    "Aggregate results for **train** and **test** splits, report core metrics for the binary *Included vs Excluded* task, then evaluate the extra fields returned *only* for items predicted as **Included**.\n",
    "\n",
    "We focus on the structured fields that are straightforward to score:\n",
    "* **domain** → matches column **Social, Behavioural or Implementation Science?**\n",
    "* **dmf_stage** → matches column **DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties**\n",
    "* **decision_type** → matches column **DMF - Are the decisions regulatory, policy, or other? Please describe the “other” if applicable.**  \n",
    "  For **decision_type** a prediction that *contains* the word **other** is considered correct when the ground‑truth field also contains **other** (ignoring any free‑text description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 1 – Imports and helpers 🔌                ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 2 – Locate outputs and datasets 🔍         ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "notebook_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "root_dir      = notebook_dir  # all_class_files/\n",
    "outputs_root  = root_dir / \"outputs\"\n",
    "datasets_dir  = root_dir.parent / \"datasets\"\n",
    "\n",
    "if not outputs_root.exists():\n",
    "    raise RuntimeError(f\"Could not find outputs directory at: {outputs_root}\")\n",
    "if not datasets_dir.exists():\n",
    "    raise RuntimeError(f\"Could not find datasets directory at: {datasets_dir}\")\n",
    "\n",
    "# every sub‑folder inside outputs/ is a model name\n",
    "model_dirs = [d for d in outputs_root.iterdir() if d.is_dir() and d.name != \"datasets\"]\n",
    "if not model_dirs:\n",
    "    raise RuntimeError(f\"No model result folders found inside '{outputs_root}/'\")\n",
    "\n",
    "print(\"Models found:\", \", \".join(d.name for d in model_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 2.5 – Load and clean ground-truth data 🗄️ ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "\n",
    "def load_ground_truth(split: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and cleans ground-truth data for a given split.\n",
    "    Returns a DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    if split not in [\"train\", \"test\"]:\n",
    "        raise ValueError(f\"Invalid split: {split}. Must be either 'train' or 'test'\")\n",
    "    \n",
    "    # load ground-truth data\n",
    "    csv_path = datasets_dir / f\"{split}_dataset.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    df_t = pd.read_csv(csv_path)\n",
    "    truth_cols = {\n",
    "        \"id\": \"id\",\n",
    "        \"label\": \"ground_truth\",\n",
    "        \"Social, Behavioural or Implementation Science?\": \"domain_gt\",\n",
    "        \"DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties\": \"dmf_stage_gt\",\n",
    "        \"DMF - Are the decisions regulatory, policy, or other? Please describe the “other” if applicable.\": \"decision_type_gt\",\n",
    "        \"IS - Does your submission include or intersect with Black, Indigenous or racialized groups?\": \"bipoc_gt\",\n",
    "        \"IS - Does your submission include or intersect with Indigenous Peoples?\": \"indigenous_gt\",\n",
    "        \"IS - Have you included Sex in your study:\": \"sex_gt\",\n",
    "        \"IS - Have you included Gender in your study?\": \"gender_gt\",\n",
    "    }\n",
    "    df_t = df_t.rename(columns=truth_cols)[list(truth_cols.values())]\n",
    "\n",
    "    # clean ground-truth data\n",
    "\n",
    "    for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "        df_t[col] = df_t[col].apply(lambda x: x.strip().lower() if isinstance(x, str) else \"\")\n",
    "\n",
    "    allowed_labels = [\"included\", \"excluded\"]\n",
    "    df_t.loc[~df_t[\"ground_truth\"].isin(allowed_labels), \"ground_truth\"] = None\n",
    "\n",
    "    allowed_domains = [\"social\", \"behavioural\", \"implementation\"]\n",
    "    df_t.loc[~df_t[\"domain_gt\"].isin(allowed_domains), \"domain_gt\"] = None\n",
    "        \n",
    "    allowed_stages = [\"identify the issue and its context\", \"assess risks and benefits\", \"identify and analyze options\", \"select a strategy\", \"implement the strategy\", \"monitor and evaluate results\", \"involve interested and affected parties\"]\n",
    "    df_t.loc[~df_t[\"dmf_stage_gt\"].isin(allowed_stages), \"dmf_stage_gt\"] = None\n",
    "\n",
    "    allowed_decision_types = [\"regulatory\", \"policy\", \"other\", \"not stated\"]        \n",
    "    df_t.loc[df_t[\"decision_type_gt\"].str.contains(\"other\"), \"decision_type_gt\"] = \"other\"\n",
    "    df_t.loc[~df_t[\"decision_type_gt\"].isin(allowed_decision_types), \"decision_type_gt\"] = None\n",
    "\n",
    "    allowed_identity = [\"yes\", \"not reported\"]       \n",
    "    for col in [\"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "        df_t.loc[df_t[col].str.contains(\"yes\"), col] = \"yes\"\n",
    "        df_t.loc[~df_t[col].isin(allowed_identity), col] = None\n",
    "\n",
    "    return df_t\n",
    "\n",
    "# Load ground-truth data for both splits\n",
    "df_train = load_ground_truth(\"train\")\n",
    "df_test = load_ground_truth(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═════════════════════════════════════════╗\n",
    "# ║ Cell 2.7 – Explore ground-truth data 🗄️ ║\n",
    "# ╚═════════════════════════════════════════╝\n",
    "\n",
    "# Look at the distribution of ground-truth labels in the training set.\n",
    "print(f\"There are {len(df_train)} rows in the training set.\")\n",
    "for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "    counts = df_train[col].value_counts(dropna=False)\n",
    "    counts_norm = df_train[col].value_counts(normalize=True) * 100\n",
    "    None_count = counts.get(None, 0)\n",
    "    print(f\"\\nThere are {None_count} None elements in the {col} column.\")\n",
    "    print(f\"If we ignore None elements, the labels in {col} are distributed as follows:\")\n",
    "    print(counts_norm.rename_axis(None).rename(None).map(lambda x: f\"{x:.1f}%\").to_string())\n",
    "\n",
    "# Look at the distribution of ground-truth labels in the test set.\n",
    "print(f\"\\n\\nThere are {len(df_test)} rows in the test set.\")\n",
    "for col in [\"ground_truth\", \"domain_gt\", \"dmf_stage_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]:\n",
    "    counts = df_test[col].value_counts(dropna=False)\n",
    "    counts_norm = df_test[col].value_counts(normalize=True) * 100\n",
    "    None_count = counts.get(None, 0)\n",
    "    print(f\"\\nThere are {None_count} None elements in the {col} column.\")\n",
    "    print(f\"If we ignore None elements, the labels in {col} are distributed as follows:\")\n",
    "    print(counts_norm.rename_axis(None).rename(None).map(lambda x: f\"{x:.1f}%\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06682c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═══════════════════════════════════════════════════════╗\n",
    "# ║ Cell 2.8 – Display ground-truth label distribution 🗄️ ║\n",
    "# ╚═══════════════════════════════════════════════════════╝\n",
    "import math\n",
    "\n",
    "cols = [\"ground_truth\", \"domain_gt\", \"decision_type_gt\", \"bipoc_gt\", \"indigenous_gt\", \"sex_gt\", \"gender_gt\"]\n",
    "num_fig_cols = math.ceil(len(cols)/2)\n",
    "fig, ax = plt.subplots(2, num_fig_cols, figsize=(20,10))\n",
    "for i, col in enumerate(cols):\n",
    "    counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "    counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "    df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "    df_counts.columns = ['train', 'test']\n",
    "    df_counts.rename_axis(None, inplace=True)\n",
    "    axis = ax[i // num_fig_cols, i % num_fig_cols]\n",
    "    df_counts.plot(kind='bar', ax=axis, rot=0)\n",
    "    axis.set_title(col)\n",
    "    axis.set_ylabel('%')\n",
    "    axis.legend(loc='upper right')\n",
    "for j in range(i+1, 2*num_fig_cols):\n",
    "    fig.delaxes(ax[j // num_fig_cols, j % num_fig_cols])  # remove empty subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "col = \"dmf_stage_gt\"  # plot label distribution separately for \"dmf_stage_gt\"        \n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(20, 5)) \n",
    "counts_train = df_train[col].value_counts(normalize=True) * 100\n",
    "counts_test = df_test[col].value_counts(normalize=True) * 100\n",
    "df_counts = pd.concat([counts_train, counts_test], axis=1)\n",
    "df_counts.columns = ['train', 'test']\n",
    "df_counts.rename_axis(None, inplace=True)\n",
    "axis = ax2\n",
    "df_counts.plot(kind='bar', ax=axis, rot=45)\n",
    "axis.set_title(col)\n",
    "axis.set_ylabel('%')\n",
    "axis.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═════════════════════════════════════════════════════╗\n",
    "# ║ Cell 3 – Load predictions and merge ground-truth 🗄️ ║\n",
    "# ╚═════════════════════════════════════════════════════╝\n",
    "# Structure: {model: {split: DataFrame}}\n",
    "all_predictions = defaultdict(dict)\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    model_name = mdir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = mdir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for jf in preds_dir.glob(\"*.json\"):\n",
    "            with open(jf, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            pred_block = data.get(\"prediction\", {})\n",
    "            x = {\n",
    "                \"id\": jf.stem,\n",
    "                \"pred_class\": pred_block.get(\"classification\"),\n",
    "                \"domain_pred\": pred_block.get(\"domain\"),\n",
    "                \"dmf_stage_pred\": pred_block.get(\"dmf_stage\"),\n",
    "                \"decision_type_pred\": pred_block.get(\"decision_type\"),\n",
    "                \"bipoc_pred\": pred_block.get(\"BIPOC\"),\n",
    "                \"indigenous_pred\": pred_block.get(\"Indigenous\"),\n",
    "                \"sex_pred\": pred_block.get(\"Sex\"),\n",
    "                \"gender_pred\": pred_block.get(\"Gender\"),\n",
    "                \"identity_factors_pred\": pred_block.get(\"Identity_Factors\"),\n",
    "            }\n",
    "            rows.append(x)\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        df_pred = pd.DataFrame(rows)\n",
    "\n",
    "        # Clean prediction data\n",
    "        for col in [\"pred_class\", \"domain_pred\", \"dmf_stage_pred\", \"decision_type_pred\", \"bipoc_pred\", \"indigenous_pred\", \"sex_pred\", \"gender_pred\", \"identity_factors_pred\"]:\n",
    "            df_pred[col] = df_pred[col].apply(lambda x: str(x).strip().lower() if x is not None else None)\n",
    "\n",
    "        if split == \"train\":\n",
    "            df_truth = df_train\n",
    "        else:\n",
    "            df_truth = df_test\n",
    "\n",
    "        df = pd.merge(df_pred, df_truth, on=\"id\", how=\"left\")\n",
    "        all_predictions[model_name][split] = df\n",
    "\n",
    "        unparsable = (df[\"pred_class\"] == \"parseerror\").sum()\n",
    "        print(f\"{model_name} [{split}] -> {len(df):,} rows, {unparsable} unparsable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 4 – Core binary‑classification metrics 📋 ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "metrics_cls = []\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"included\", \"excluded\"])]\n",
    "        unparsed = len(df) - len(parsable)\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        metrics_cls.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"n_total\": len(df),\n",
    "                \"n_unparsed\": unparsed,\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred, pos_label=\"included\"),\n",
    "                \"recall\": recall_score(y_true, y_pred, pos_label=\"included\"),\n",
    "                \"f1\": f1_score(y_true, y_pred, pos_label=\"included\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "metrics_cls_df = (\n",
    "    pd.DataFrame(metrics_cls)\n",
    "    .set_index([\"model\", \"split\"])\n",
    "    .sort_values([\"model\", \"split\"])\n",
    ")\n",
    "metrics_cls_df.style.format({\"accuracy\": \"{:.3f}\", \"precision\": \"{:.3f}\", \"recall\": \"{:.3f}\", \"f1\": \"{:.3f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f849f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 5 – Visualise binary metrics 📊           ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for split in (\"train\", \"test\"):\n",
    "    subset = metrics_cls_df.xs(split, level=\"split\")\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    subset[\"accuracy\"].plot(kind=\"bar\", ax=axes[0])\n",
    "    axes[0].set_title(f\"Accuracy ({split})\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    subset[\"f1\"].plot(kind=\"bar\", ax=axes[1])\n",
    "    axes[1].set_title(f\"F1‑score ({split})\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    (subset[\"n_unparsed\"] / subset[\"n_total\"]).plot(kind=\"bar\", ax=axes[2])\n",
    "    axes[2].set_title(f\"Unparsed % ({split})\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(f\"Model comparison on {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 6 – Extra-field scoring 🏷️               ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "extra_metrics = []\n",
    "fields = [\n",
    "    (\"domain_pred\",       \"domain_gt\",       \"domain\"),\n",
    "    (\"dmf_stage_pred\",    \"dmf_stage_gt\",    \"dmf_stage\"),\n",
    "    (\"decision_type_pred\",\"decision_type_gt\",\"decision_type\"),\n",
    "    (\"bipoc_pred\",        \"bipoc_gt\",        \"bipoc\"),\n",
    "    (\"indigenous_pred\",   \"indigenous_gt\",   \"indigenous\"),\n",
    "    (\"sex_pred\",          \"sex_gt\",          \"sex\"),\n",
    "    (\"gender_pred\",       \"gender_gt\",       \"gender\")\n",
    "]\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "\n",
    "        # evaluate only items predicted as \"included\"  \n",
    "        include_rows = df[\"pred_class\"] == \"included\"\n",
    "\n",
    "        # do not evaluate items for which ground truth label is 'included' but other ground truth fields are None\n",
    "        if fields:\n",
    "            conditions = [df[\"ground_truth\"] == \"included\"]\n",
    "            for field in fields:\n",
    "                conditions.append(df[field[1]].isna())\n",
    "            exclude_rows = pd.concat(conditions, axis=1).all(axis=1)\n",
    "            df_inc = df[include_rows & ~exclude_rows].copy()\n",
    "        else:\n",
    "            df_inc = df[include_rows].copy()\n",
    "\n",
    "        if df_inc.empty:\n",
    "            continue\n",
    "\n",
    "        for pcol, tcol, name in fields:\n",
    "            \n",
    "            # exclude rows where ground_truth label is \"included\" and value in tcol equals None\n",
    "            excl = (df_inc[\"ground_truth\"] == \"included\") & df_inc[tcol].isna()\n",
    "\n",
    "            if (~excl).sum() > 0:\n",
    "                acc = accuracy_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''))\n",
    "                cl = df_inc.loc[~excl, tcol].fillna('').unique()\n",
    "                class_labels = [x for x in cl if x]  # remove empty string from class labels                        \n",
    "                f1 = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average='macro', labels=class_labels)\n",
    "                #f1_weighted = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average='weighted', labels=class_labels)\n",
    "                #f1_none = f1_score(df_inc.loc[~excl, tcol].fillna(''), df_inc.loc[~excl, pcol].fillna(''), average=None, labels=class_labels)\n",
    "                #print(f\"{model} {split} {name} f1_macro: {f1:.3f} f1_weighted: {f1_weighted:.3f}\")\n",
    "                #print(f\"{class_labels} : {f1_none}\")                \n",
    "            else:\n",
    "                acc = float(\"nan\")\n",
    "                f1 = float(\"nan\")\n",
    "\n",
    "            extra_metrics.append(\n",
    "                {\n",
    "                    \"model\"   : model,\n",
    "                    \"split\"   : split,\n",
    "                    \"field\"   : name,\n",
    "                    \"n_scored\": (~excl).sum(),\n",
    "                    \"accuracy\": round(acc, 3),\n",
    "                    \"f1\"      : round(f1, 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# ---- pivot & pretty-print ---- #\n",
    "a_extra = pd.DataFrame(extra_metrics)\n",
    "if not a_extra.empty:\n",
    "    pivot_acc = a_extra.pivot(index=[\"model\", \"split\"], columns=\"field\", values=\"accuracy\")\n",
    "    pivot_acc = pivot_acc[[\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]]\n",
    "    print('Accuracy:')\n",
    "    display(pivot_acc.style.format(\"{:.3f}\"))\n",
    "    pivot_f1 = a_extra.pivot(index=[\"model\", \"split\"], columns=\"field\", values=\"f1\")\n",
    "    pivot_f1 = pivot_f1[[\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]]\n",
    "    print('F1‑score:')\n",
    "    display(pivot_f1.style.format(\"{:.3f}\"))\n",
    "else:\n",
    "    print(\"Cannot score extra fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║ Cell 7 – Visualise extra-field accuracy and f1 score 📊🏷️   ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "if not a_extra.empty:\n",
    "    for field in [\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]:\n",
    "        fig, ax = plt.subplots(figsize=(9, 4))\n",
    "        subset_acc = a_extra[a_extra[\"field\"] == field].pivot(\n",
    "            index=\"model\", columns=\"split\", values=\"accuracy\"\n",
    "        )\n",
    "        subset_acc.plot(kind=\"bar\", ax=ax, rot=0)  # one bar-group per model\n",
    "        ax.set_title(f\"{field} – accuracy by model and split\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"model\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(9, 4))\n",
    "        subset_f1 = a_extra[a_extra[\"field\"] == field].pivot(\n",
    "            index=\"model\", columns=\"split\", values=\"f1\"\n",
    "        )\n",
    "        subset_f1.plot(kind=\"bar\", ax=ax2, rot=0)  # one bar-group per model\n",
    "        ax2.set_title(f\"{field} – f1 score by model and split\")\n",
    "        ax2.set_ylabel(\"f1 score\")\n",
    "        ax2.set_xlabel(\"model\")\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═════════════════════════════════════════════════════════════╗\n",
    "# ║ Cell 8 – Per-model feature accuracy and f1 score 📊         ║\n",
    "# ╚═════════════════════════════════════════════════════════════╝\n",
    "if not a_extra.empty:\n",
    "    for model_name in a_extra[\"model\"].unique():\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        subset_acc = (\n",
    "            a_extra[a_extra[\"model\"] == model_name]\n",
    "            .pivot(index=\"field\", columns=\"split\", values=\"accuracy\")\n",
    "            .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "        )\n",
    "        subset_acc.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_title(f\"{model_name} – extra-field accuracy\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"field\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "        subset_f1 = (\n",
    "            a_extra[a_extra[\"model\"] == model_name]\n",
    "            .pivot(index=\"field\", columns=\"split\", values=\"f1\")\n",
    "            .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "        )\n",
    "        subset_f1.plot(kind=\"bar\", ax=ax2, rot=0)\n",
    "        ax2.set_title(f\"{model_name} – extra-field f1 score\")\n",
    "        ax2.set_ylabel(\"f1 score\")\n",
    "        ax2.set_xlabel(\"field\")\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0173cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 9 – Confusion matrices for binary task 🔲 ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"included\", \"excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"included\", \"excluded\"])\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Purples\",\n",
    "            xticklabels=[\"Included\", \"Excluded\"],\n",
    "            yticklabels=[\"Included\", \"Excluded\"],\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix – {model} ({split})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Classification report for {model} ({split})\")\n",
    "        print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═════════════════════════════════════════════════╗\n",
    "# ║ Cell 10 - Automated Free‑text field evaluation  ║\n",
    "# ╚═════════════════════════════════════════════════╝\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# load Azure settings\n",
    "load_dotenv()   # expects ENDPOINT_URL, AZURE_OPENAI_API_KEY, DEPLOYMENT_NAME, AZURE_OPENAI_API_VERSION\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"ENDPOINT_URL\"),\n",
    "    api_version    = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\"),\n",
    ")\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "# regex to strip code fences\n",
    "fence_re = re.compile(r\"^```(?:json)?\\s*|\\s*```$\", flags=re.MULTILINE)\n",
    "\n",
    "ft_metrics = []\n",
    "outputs_root = Path.cwd() / \"outputs\"\n",
    "\n",
    "for model_dir in outputs_root.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    model = model_dir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = model_dir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "        preds = sorted(preds_dir.glob(\"*.json\"))\n",
    "\n",
    "        for jf in tqdm(preds, desc=f\"eval free-text {model} {split}\"):\n",
    "            rec = json.loads(jf.read_text(encoding=\"utf-8\"))\n",
    "            gt  = rec[\"ground_truth\"]\n",
    "            pr  = rec[\"prediction\"]\n",
    "\n",
    "            # require both truth and prediction to be 'Included'\n",
    "            cls_pred = pr.get(\"classification\",\"\")\n",
    "            cls_truth = gt.get(\"classification\",\"\")\n",
    "            if not (isinstance(cls_pred, str) and cls_pred.strip().lower()==\"included\"):\n",
    "                continue\n",
    "            if not (isinstance(cls_truth, str) and cls_truth.strip().lower()==\"included\"):\n",
    "                continue\n",
    "\n",
    "            # extract ground-truth free-text fields\n",
    "            aud_gt = gt.get(\"audience\",\"\") or \"\"\n",
    "            meth_gt = gt.get(\"methodology\",\"\") or \"\"\n",
    "            sz_raw = gt.get(\"sample_size\", None)\n",
    "            sz_gt = str(sz_raw).strip() if sz_raw not in (None, \"\") else \"\"\n",
    "            idfac_gt = gt.get(\"identity_factors_gt\", \"\") or \"\"\n",
    "\n",
    "            # skip if all truth fields are blank\n",
    "            if not any([aud_gt.strip(), meth_gt.strip(), sz_gt, idfac_gt.strip()]):\n",
    "                continue\n",
    "\n",
    "            # extract predicted free-text fields\n",
    "            aud_pr = pr.get(\"audience\",\"\") or \"\"\n",
    "            meth_pr = pr.get(\"methodology\",\"\") or \"\"\n",
    "            sz_pr = str(pr.get(\"sample_size\",\"\")).strip()\n",
    "            idfac_pr = pr.get(\"Identity_Factors\", \"\") or \"\"\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "You are an evaluator of free-text fields. Compare these pairs:\n",
    "\n",
    "Ground truth Audience: {aud_gt}\n",
    "Predicted Audience: {aud_pr}\n",
    "\n",
    "Ground truth Methodology: {meth_gt}\n",
    "Predicted Methodology: {meth_pr}\n",
    "\n",
    "Ground truth Sample Size: {sz_gt}\n",
    "Predicted Sample Size: {sz_pr}\n",
    "\n",
    "Ground truth Identity Factors: {idfac_gt}\n",
    "Predicted Identity Factors: {idfac_pr}\n",
    "\n",
    "Be lenient and forgiving:\n",
    "- Audience is correct if the core demographic appears in the predicted text even with extra qualifiers\n",
    "- Methodology is correct if it’s semantically equivalent or more specific\n",
    "- Sample Size is correct if numeric values match ignoring formatting (e.g. \"n=20\" vs \"20\")\n",
    "- Identity Factors is correct if core identity considerations in ground-truth appear in the prediction even with extra qualifiers\n",
    "\n",
    "Reply in JSON format like this:\n",
    "\n",
    "{{\n",
    "  \"audience_correct\": 0_or_1,\n",
    "  \"methodology_correct\": 0_or_1,\n",
    "  \"sample_size_correct\": 0_or_1\n",
    "  \"identity_factors_correct\": 0_or_1\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "            # call LLM with up to 5 retries for valid JSON\n",
    "            comp = {\"audience_correct\": None, \"methodology_correct\": None, \"sample_size_correct\": None, \"identity_factors_correct\": None}\n",
    "            for _ in range(5):\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=DEPLOYMENT_NAME,\n",
    "                    messages=[\n",
    "                        {\"role\":\"system\",\"content\":\"You are a helpful assistant that evaluates the similarity of pairs of free-text fields and responds in JSON format.\"},\n",
    "                        {\"role\":\"user\",  \"content\":prompt}\n",
    "                    ],\n",
    "                    temperature=0\n",
    "                )\n",
    "                text = fence_re.sub(\"\", resp.choices[0].message.content.strip())\n",
    "                try:\n",
    "                    comp = json.loads(text)\n",
    "                    break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "            ft_metrics.append({\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"id\":    jf.stem,\n",
    "                \"audience_gt\":        aud_gt,\n",
    "                \"audience_pred\":      aud_pr,\n",
    "                \"audience_correct\":   comp.get(\"audience_correct\", None),\n",
    "                \"methodology_gt\":     meth_gt,\n",
    "                \"methodology_pred\":   meth_pr,\n",
    "                \"methodology_correct\":comp.get(\"methodology_correct\",None),\n",
    "                \"sample_size_gt\":     sz_gt,\n",
    "                \"sample_size_pred\":   sz_pr,\n",
    "                \"sample_size_correct\":comp.get(\"sample_size_correct\", None),\n",
    "                \"identity_factors_gt\": idfac_gt,\n",
    "                \"identity_factors_pred\": idfac_pr,\n",
    "                \"identity_factors_correct\": comp.get(\"identity_factors_correct\", None),\n",
    "            })\n",
    "\n",
    "# build DataFrame\n",
    "df_ft = pd.DataFrame(ft_metrics)\n",
    "\n",
    "# save side‑by‑side Excel per model/split\n",
    "for (model, split), group in df_ft.groupby([\"model\", \"split\"]):\n",
    "    out_dir = outputs_root / model / split\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = out_dir / f\"free_text_eval_{model}.xlsx\"\n",
    "    group[[\n",
    "        \"id\",\n",
    "        \"audience_gt\",\"audience_pred\",\"audience_correct\",\n",
    "        \"methodology_gt\",\"methodology_pred\",\"methodology_correct\",\n",
    "        \"sample_size_gt\",\"sample_size_pred\",\"sample_size_correct\",\n",
    "        \"identity_factors_gt\",\"identity_factors_pred\",\"identity_factors_correct\"\n",
    "    ]].to_excel(file_path, index=False)\n",
    "\n",
    "# display summary pivot\n",
    "if not df_ft.empty:\n",
    "    summary = (\n",
    "        df_ft\n",
    "        .melt(\n",
    "            id_vars=[\"model\",\"split\",\"id\"],\n",
    "            value_vars=[\"audience_correct\",\"methodology_correct\",\"sample_size_correct\",\"identity_factors_correct\"],\n",
    "            var_name=\"field\",\n",
    "            value_name=\"correct\"\n",
    "        )\n",
    "        .groupby([\"model\",\"split\",\"field\"])\n",
    "        .agg(n_scored=(\"correct\",\"size\"), accuracy=(\"correct\",\"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    display(\n",
    "        summary\n",
    "        .pivot(index=[\"model\",\"split\"], columns=\"field\", values=\"accuracy\")\n",
    "        .style.format(\"{:.3f}\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No Included records with free-text truth to evaluate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
