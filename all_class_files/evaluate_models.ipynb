{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de73e4a",
   "metadata": {},
   "source": [
    "# 📊🧪 Literature Screening – Multi‑class Model Evaluation\n",
    "Aggregate results for **train** and **test** splits, report core metrics for the binary *Included vs Excluded* task, then evaluate the extra fields returned *only* for items predicted as **Included**.\n",
    "\n",
    "We focus on the structured fields that are straightforward to score:\n",
    "* **domain** → matches column **Social, Behavioural or Implementation Science?**\n",
    "* **dmf_stage** → matches column **DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties**\n",
    "* **decision_type** → matches column **DMF - Are the decisions regulatory, policy, or other? Please describe the “other” if applicable.**  \n",
    "  For **decision_type** a prediction that *contains* the word **other** is considered correct when the ground‑truth field also contains **other** (ignoring any free‑text description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 1 – Imports and helpers 🔌                ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# basic stop-word list (tune as needed)\n",
    "_STOPWORDS: set[str] = {\n",
    "    \"the\", \"a\", \"an\", \"of\", \"and\", \"to\", \"in\", \"on\", \"for\", \"with\",\n",
    "    \"at\", \"by\", \"from\", \"about\", \"as\", \"into\", \"that\", \"this\",\n",
    "}\n",
    "\n",
    "_token_re = re.compile(r\"[a-z0-9]+\")\n",
    "\n",
    "\n",
    "# ── text normalisation helpers ─────────────────── #\n",
    "def _ascii_fold(text: str) -> str:\n",
    "    \"\"\"Transliterate accented characters → plain ASCII.\"\"\"\n",
    "    return (\n",
    "        unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"ascii\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalisation that ignores superficial differences:\n",
    "      • accents, case, punctuation, apostrophes\n",
    "      • extra whitespace / newlines\n",
    "      • common stop-words\n",
    "      • token order (tokens are deduped & sorted)\n",
    "    Returns a single space-separated string.\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "        return \"\"\n",
    "\n",
    "    txt = _ascii_fold(str(text).lower())\n",
    "    txt = txt.replace(\"’\", \" \").replace(\"'\", \" \")\n",
    "\n",
    "    tokens: Iterable[str] = _token_re.findall(txt)\n",
    "    tokens = [t for t in tokens if t not in _STOPWORDS]\n",
    "\n",
    "    # deduplicate and sort so order does not matter\n",
    "    tokens = sorted(set(tokens))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# ── fuzzy comparison wrappers ──────────────────── #\n",
    "def _fuzzy_equal(a: str, b: str, threshold: int = 90) -> bool:\n",
    "    \"\"\"\n",
    "    Fuzzy equality using rapidfuzz (ratio 0-100).\n",
    "    Falls back to strict equality if rapidfuzz not available.\n",
    "    \"\"\"\n",
    "    if fuzz is None:\n",
    "        return _normalize(a) == _normalize(b)\n",
    "    return fuzz.ratio(_normalize(a), _normalize(b)) >= threshold\n",
    "\n",
    "\n",
    "def _match_decision_type(pred: str, truth: str) -> bool:\n",
    "    \"\"\"\n",
    "    Decision-type matching with relaxed “other” rule:\n",
    "      • if ground truth mentions “other”, prediction is correct\n",
    "        when it also contains “other” (after normalisation)\n",
    "      • otherwise, use fuzzy equality\n",
    "    \"\"\"\n",
    "    if pd.isna(truth):\n",
    "        return False\n",
    "\n",
    "    p_norm = _normalize(pred)\n",
    "    t_norm = _normalize(truth)\n",
    "\n",
    "    if \"other\" in t_norm and \"other\" in p_norm:\n",
    "        return True\n",
    "    return _fuzzy_equal(p_norm, t_norm, threshold=92)\n",
    "\n",
    "def _match_identity(pred: str, truth: str) -> bool:\n",
    "    \"\"\"\n",
    "    String matching with relaxed rules - used for bipoc, indigenous, sex, and gender fields \n",
    "    If the two strings contain \"yes\", return True.\n",
    "    If the two strings contain \"not reported\", return True.\n",
    "    Otherwise, compare the normalized strings.\n",
    "    \"\"\"\n",
    "    truth_l = truth.lower() if isinstance(truth, str) else \"\"\n",
    "    pred_l = pred.lower() if isinstance(pred, str) else \"\"\n",
    "\n",
    "    if (\"yes\" in truth_l) and (\"yes\" in pred_l):\n",
    "        return True\n",
    "    elif (\"not reported\" in truth_l) and (\"not reported\" in pred_l):\n",
    "        return True\n",
    "    else:\n",
    "        return(_normalize(pred) == _normalize(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 2 – Locate outputs and datasets 🔍         ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "notebook_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "root_dir      = notebook_dir  # all_class_files/\n",
    "outputs_root  = root_dir / \"outputs\"\n",
    "datasets_dir  = root_dir.parent / \"datasets\"\n",
    "\n",
    "if not outputs_root.exists():\n",
    "    raise RuntimeError(f\"Could not find outputs directory at: {outputs_root}\")\n",
    "if not datasets_dir.exists():\n",
    "    raise RuntimeError(f\"Could not find datasets directory at: {datasets_dir}\")\n",
    "\n",
    "# every sub‑folder inside outputs/ is a model name\n",
    "model_dirs = [d for d in outputs_root.iterdir() if d.is_dir() and d.name != \"datasets\"]\n",
    "if not model_dirs:\n",
    "    raise RuntimeError(f\"No model result folders found inside '{outputs_root}/'\")\n",
    "\n",
    "print(\"Models found:\", \", \".join(d.name for d in model_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 3 – Load predictions and merge ground-truth 🗄️ ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "# Structure: {model: {split: DataFrame}}\n",
    "all_predictions = defaultdict(dict)\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    model_name = mdir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = mdir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # ----- ground-truth dataset ----- #\n",
    "        csv_path = datasets_dir / f\"{split}_dataset.csv\"\n",
    "        if not csv_path.exists():\n",
    "            raise FileNotFoundError(csv_path)\n",
    "        df_truth = pd.read_csv(csv_path)\n",
    "        truth_cols = {\n",
    "            \"id\": \"id\",\n",
    "            \"label\": \"ground_truth\",\n",
    "            \"Social, Behavioural or Implementation Science?\": \"domain_gt\",\n",
    "            \"DMF - Identify the issue and its context, assess risks and benefits, identify and analyze options, select a strategy, implement the strategy, monitor and evaluate results, involve interested and affected parties\": \"dmf_stage_gt\",\n",
    "            \"DMF - Are the decisions regulatory, policy, or other? Please describe the “other” if applicable.\": \"decision_type_gt\",\n",
    "            \"IS - Does your submission include or intersect with Black, Indigenous or racialized groups?\": \"bipoc_gt\",\n",
    "            \"IS - Does your submission include or intersect with Indigenous Peoples?\": \"indigenous_gt\",\n",
    "            \"IS - Have you included Sex in your study:\": \"sex_gt\",\n",
    "            \"IS - Have you included Gender in your study?\": \"gender_gt\",\n",
    "            \"IS - Have you considered identity factors other than sex and gender?\": \"identity_factors_gt\",\n",
    "        }\n",
    "        df_truth = df_truth.rename(columns=truth_cols)[list(truth_cols.values())]\n",
    "\n",
    "        # ----- predictions ----- #\n",
    "        rows = []\n",
    "        for jf in preds_dir.glob(\"*.json\"):\n",
    "            with open(jf, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            pred_block = data.get(\"prediction\", {})\n",
    "            x = {\n",
    "                \"id\": jf.stem,\n",
    "                \"pred_class\": pred_block.get(\"classification\"),\n",
    "                \"raw_rationale\": pred_block.get(\"classification_rationale\"),\n",
    "                \"domain_pred\": pred_block.get(\"domain\"),\n",
    "                \"dmf_stage_pred\": pred_block.get(\"dmf_stage\"),\n",
    "                \"decision_type_pred\": pred_block.get(\"decision_type\"),\n",
    "                \"bipoc_pred\": pred_block.get(\"BIPOC\"),\n",
    "                \"indigenous_pred\": pred_block.get(\"Indigenous\"),\n",
    "                \"sex_pred\": pred_block.get(\"Sex\"),\n",
    "                \"gender_pred\": pred_block.get(\"Gender\"),\n",
    "            }\n",
    "            rows.append(x)\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        df_pred = pd.DataFrame(rows)\n",
    "        df = pd.merge(df_pred, df_truth, on=\"id\", how=\"left\")\n",
    "        all_predictions[model_name][split] = df\n",
    "\n",
    "        unparsable = (df[\"pred_class\"] == \"ParseError\").sum()\n",
    "        print(f\"{model_name} [{split}] -> {len(df):,} rows, {unparsable} unparsable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 4 – Core binary‑classification metrics 📋 ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "metrics_cls = []\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"Included\", \"Excluded\"])]\n",
    "        unparsed = len(df) - len(parsable)\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        metrics_cls.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"n_total\": len(df),\n",
    "                \"n_unparsed\": unparsed,\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred, pos_label=\"Included\"),\n",
    "                \"recall\": recall_score(y_true, y_pred, pos_label=\"Included\"),\n",
    "                \"f1\": f1_score(y_true, y_pred, pos_label=\"Included\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "metrics_cls_df = (\n",
    "    pd.DataFrame(metrics_cls)\n",
    "    .set_index([\"model\", \"split\"])\n",
    "    .sort_values([\"model\", \"split\"])\n",
    ")\n",
    "metrics_cls_df.style.format({\"accuracy\": \"{:.3f}\", \"precision\": \"{:.3f}\", \"recall\": \"{:.3f}\", \"f1\": \"{:.3f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f849f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 5 – Visualise binary metrics 📊           ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for split in (\"train\", \"test\"):\n",
    "    subset = metrics_cls_df.xs(split, level=\"split\")\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    subset[\"accuracy\"].plot(kind=\"bar\", ax=axes[0])\n",
    "    axes[0].set_title(f\"Accuracy ({split})\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    subset[\"f1\"].plot(kind=\"bar\", ax=axes[1])\n",
    "    axes[1].set_title(f\"F1‑score ({split})\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    (subset[\"n_unparsed\"] / subset[\"n_total\"]).plot(kind=\"bar\", ax=axes[2])\n",
    "    axes[2].set_title(f\"Unparsed % ({split})\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(f\"Model comparison on {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 6 – Extra-field scoring 🏷️               ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "extra_metrics = []\n",
    "fields = [\n",
    "    (\"domain_pred\",       \"domain_gt\",       \"domain\"),\n",
    "    (\"dmf_stage_pred\",    \"dmf_stage_gt\",    \"dmf_stage\"),\n",
    "    (\"decision_type_pred\",\"decision_type_gt\",\"decision_type\"),\n",
    "    (\"bipoc_pred\",        \"bipoc_gt\",        \"bipoc\"),\n",
    "    (\"indigenous_pred\",   \"indigenous_gt\",   \"indigenous\"),\n",
    "    (\"sex_pred\",          \"sex_gt\",          \"sex\"),\n",
    "    (\"gender_pred\",       \"gender_gt\",       \"gender\")\n",
    "]\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "\n",
    "        # evaluate only items predicted as \"Included\"  \n",
    "        include_rows = df[\"pred_class\"] == \"Included\"\n",
    "\n",
    "        # do not evaluate items for which ground truth label is 'Included' but other ground truth fields are None\n",
    "        if fields:\n",
    "            conditions = [df[\"ground_truth\"] == \"Included\"]\n",
    "            for field in fields:\n",
    "                conditions.append(df[field[1]].isna())\n",
    "            exclude_rows = pd.concat(conditions, axis=1).all(axis=1)\n",
    "            df_inc = df[include_rows & ~exclude_rows].copy()\n",
    "        else:\n",
    "            df_inc = df[include_rows].copy()\n",
    "\n",
    "        if df_inc.empty:\n",
    "            continue\n",
    "\n",
    "        for pcol, tcol, name in fields:\n",
    "            if name == \"decision_type\":\n",
    "                matches = [_match_decision_type(p, t) for p, t in zip(df_inc[pcol], df_inc[tcol])]\n",
    "            elif name in [\"bipoc\", \"indigenous\", \"sex\", \"gender\"]:\n",
    "                matches = [_match_identity(p, t) for p, t in zip(df_inc[pcol], df_inc[tcol])]\n",
    "            else:\n",
    "                matches = [_normalize(p) == _normalize(t) for p, t in zip(df_inc[pcol], df_inc[tcol])]\n",
    "\n",
    "            acc = float(np.mean(matches)) if matches else float(\"nan\")\n",
    "    \n",
    "            extra_metrics.append(\n",
    "                {\n",
    "                    \"model\"   : model,\n",
    "                    \"split\"   : split,\n",
    "                    \"field\"   : name,\n",
    "                    \"n_scored\": int(len(matches)),\n",
    "                    \"accuracy\": round(acc, 3),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# ---- pivot & pretty-print ---- #\n",
    "a_extra = pd.DataFrame(extra_metrics)\n",
    "if not a_extra.empty:\n",
    "    pivot = a_extra.pivot(index=[\"model\", \"split\"], columns=\"field\", values=\"accuracy\")\n",
    "    pivot = pivot[[\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]]\n",
    "    display(pivot.style.format(\"{:.3f}\"))\n",
    "else:\n",
    "    print(\"No Included predictions found, cannot score extra fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 7 – Visualise extra-field accuracy 📊🏷️   ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "if not a_extra.empty:\n",
    "    for field in [\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"]:\n",
    "        fig, ax = plt.subplots(figsize=(9, 4))\n",
    "        subset = a_extra[a_extra[\"field\"] == field].pivot(\n",
    "            index=\"model\", columns=\"split\", values=\"accuracy\"\n",
    "        )\n",
    "        subset.plot(kind=\"bar\", ax=ax, rot=0)  # one bar-group per model\n",
    "        ax.set_title(f\"{field} – accuracy by model and split\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"model\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 8 – Per-model feature accuracy 📊         ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "if not a_extra.empty:\n",
    "    for model_name in a_extra[\"model\"].unique():\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        subset = (\n",
    "            a_extra[a_extra[\"model\"] == model_name]\n",
    "            .pivot(index=\"field\", columns=\"split\", values=\"accuracy\")\n",
    "            .reindex([\"domain\", \"dmf_stage\", \"decision_type\", \"bipoc\", \"indigenous\", \"sex\", \"gender\"])\n",
    "        )\n",
    "        subset.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_title(f\"{model_name} – extra-field accuracy\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "        ax.set_xlabel(\"field\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title=\"split\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No extra-field metrics to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0173cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 9 – Confusion matrices for binary task 🔲 ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"pred_class\"].isin([\"Included\", \"Excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"pred_class\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"Included\", \"Excluded\"])\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Purples\",\n",
    "            xticklabels=[\"Included\", \"Excluded\"],\n",
    "            yticklabels=[\"Included\", \"Excluded\"],\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix – {model} ({split})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Classification report for {model} ({split})\")\n",
    "        print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔═════════════════════════════════════════════════╗\n",
    "# ║ Cell 10 - Automated Free‑text field evaluation  ║\n",
    "# ╚═════════════════════════════════════════════════╝\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# load Azure settings\n",
    "load_dotenv()   # expects ENDPOINT_URL, AZURE_OPENAI_API_KEY, DEPLOYMENT_NAME, AZURE_OPENAI_API_VERSION\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"ENDPOINT_URL\"),\n",
    "    api_version    = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\"),\n",
    ")\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "# regex to strip code fences\n",
    "fence_re = re.compile(r\"^```(?:json)?\\s*|\\s*```$\", flags=re.MULTILINE)\n",
    "\n",
    "ft_metrics = []\n",
    "outputs_root = Path.cwd() / \"outputs\"\n",
    "\n",
    "for model_dir in outputs_root.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    model = model_dir.name\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = model_dir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "        preds = sorted(preds_dir.glob(\"*.json\"))\n",
    "\n",
    "        for jf in tqdm(preds, desc=f\"eval free-text {model} {split}\"):\n",
    "            rec = json.loads(jf.read_text(encoding=\"utf-8\"))\n",
    "            gt  = rec[\"ground_truth\"]\n",
    "            pr  = rec[\"prediction\"]\n",
    "\n",
    "            # require both truth and prediction to be 'Included'\n",
    "            cls_pred = pr.get(\"classification\",\"\")\n",
    "            cls_truth = gt.get(\"classification\",\"\")\n",
    "            if not (isinstance(cls_pred, str) and cls_pred.strip().lower()==\"included\"):\n",
    "                continue\n",
    "            if not (isinstance(cls_truth, str) and cls_truth.strip().lower()==\"included\"):\n",
    "                continue\n",
    "\n",
    "            # extract ground-truth free-text fields\n",
    "            aud_gt = gt.get(\"audience\",\"\") or \"\"\n",
    "            meth_gt = gt.get(\"methodology\",\"\") or \"\"\n",
    "            sz_raw = gt.get(\"sample_size\", None)\n",
    "            sz_gt = str(sz_raw).strip() if sz_raw not in (None, \"\") else \"\"\n",
    "\n",
    "            # skip if all truth fields are blank\n",
    "            if not any([aud_gt.strip(), meth_gt.strip(), sz_gt]):\n",
    "                continue\n",
    "\n",
    "            # extract predicted free-text fields\n",
    "            aud_pr = pr.get(\"audience\",\"\") or \"\"\n",
    "            meth_pr = pr.get(\"methodology\",\"\") or \"\"\n",
    "            sz_pr = str(pr.get(\"sample_size\",\"\")).strip()\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "You are an evaluator of free-text fields. Compare these pairs:\n",
    "\n",
    "Ground truth Audience: {aud_gt}\n",
    "Predicted Audience: {aud_pr}\n",
    "\n",
    "Ground truth Methodology: {meth_gt}\n",
    "Predicted Methodology: {meth_pr}\n",
    "\n",
    "Ground truth Sample Size: {sz_gt}\n",
    "Predicted Sample Size: {sz_pr}\n",
    "\n",
    "Be lenient and forgiving:\n",
    "- Audience is correct if the core demographic appears in the predicted text even with extra qualifiers\n",
    "- Methodology is correct if it’s semantically equivalent or more specific\n",
    "- Sample Size is correct if numeric values match ignoring formatting (e.g. \"n=20\" vs \"20\")\n",
    "\n",
    "Return only this JSON, nothing else:\n",
    "\n",
    "{{\n",
    "  \"audience_correct\": 0_or_1,\n",
    "  \"methodology_correct\": 0_or_1,\n",
    "  \"sample_size_correct\": 0_or_1\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "            # call LLM with up to 5 retries for valid JSON\n",
    "            comp = {\"audience_correct\":0, \"methodology_correct\":0, \"sample_size_correct\":0}\n",
    "            for _ in range(5):\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=DEPLOYMENT_NAME,\n",
    "                    messages=[\n",
    "                        {\"role\":\"system\",\"content\":\"You evaluate free-text fields leniently and output only raw JSON\"},\n",
    "                        {\"role\":\"user\",  \"content\":prompt}\n",
    "                    ],\n",
    "                    temperature=0\n",
    "                )\n",
    "                text = fence_re.sub(\"\", resp.choices[0].message.content.strip())\n",
    "                try:\n",
    "                    comp = json.loads(text)\n",
    "                    break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "            ft_metrics.append({\n",
    "                \"model\": model,\n",
    "                \"split\": split,\n",
    "                \"id\":    jf.stem,\n",
    "                \"audience_gt\":        aud_gt,\n",
    "                \"audience_pred\":      aud_pr,\n",
    "                \"audience_correct\":   comp.get(\"audience_correct\",0),\n",
    "                \"methodology_gt\":     meth_gt,\n",
    "                \"methodology_pred\":   meth_pr,\n",
    "                \"methodology_correct\":comp.get(\"methodology_correct\",0),\n",
    "                \"sample_size_gt\":     sz_gt,\n",
    "                \"sample_size_pred\":   sz_pr,\n",
    "                \"sample_size_correct\":comp.get(\"sample_size_correct\",0),\n",
    "            })\n",
    "\n",
    "# build DataFrame\n",
    "df_ft = pd.DataFrame(ft_metrics)\n",
    "\n",
    "# save side‑by‑side Excel per model/split\n",
    "for (model, split), group in df_ft.groupby([\"model\", \"split\"]):\n",
    "    out_dir = outputs_root / model / split\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = out_dir / f\"free_text_eval_{model}.xlsx\"\n",
    "    group[[\n",
    "        \"id\",\n",
    "        \"audience_gt\",\"audience_pred\",\"audience_correct\",\n",
    "        \"methodology_gt\",\"methodology_pred\",\"methodology_correct\",\n",
    "        \"sample_size_gt\",\"sample_size_pred\",\"sample_size_correct\"\n",
    "    ]].to_excel(file_path, index=False)\n",
    "\n",
    "# display summary pivot\n",
    "if not df_ft.empty:\n",
    "    summary = (\n",
    "        df_ft\n",
    "        .melt(\n",
    "            id_vars=[\"model\",\"split\",\"id\"],\n",
    "            value_vars=[\"audience_correct\",\"methodology_correct\",\"sample_size_correct\"],\n",
    "            var_name=\"field\",\n",
    "            value_name=\"correct\"\n",
    "        )\n",
    "        .groupby([\"model\",\"split\",\"field\"])\n",
    "        .agg(n_scored=(\"correct\",\"size\"), accuracy=(\"correct\",\"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    display(\n",
    "        summary\n",
    "        .pivot(index=[\"model\",\"split\"], columns=\"field\", values=\"accuracy\")\n",
    "        .style.format(\"{:.3f}\")\n",
    "    )\n",
    "else:\n",
    "    print(\"No Included records with free-text truth to evaluate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
