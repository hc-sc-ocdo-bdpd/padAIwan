{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 1 – Imports, paths, reproducibility 🔧📂  ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- environment ---\n",
    "load_dotenv()                                      # ENDPOINT_URL, DEPLOYMENT_NAME, AZURE_OPENAI_API_KEY\n",
    "DEPLOYMENT_NAME  = os.getenv(\"DEPLOYMENT_NAME\", \"\").strip()\n",
    "AZ_VERSION       = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "\n",
    "# --- directories / files ---\n",
    "PROMPT_V1_PATH   = Path(\"prompts/v4.txt\")\n",
    "DATASET_PATH     = Path(\"outputs/datasets/train_dataset.csv\")   # always use TRAIN here\n",
    "MISCLASS_DIR     = Path(\"outputs\") / \"gpt-4.1\" / \"train\" / \"misclassified\"\n",
    "PROMPT_V2_PATH   = Path(\"prompts/v5.txt\")                       # new prompt will be written here\n",
    "\n",
    "for p in [PROMPT_V1_PATH, DATASET_PATH, MISCLASS_DIR]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing expected path: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 2 – Collect misclassified examples 🗂️    ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "# Load train split so we can attach full title/abstract\n",
    "train_df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "def load_example(json_path: Path) -> Dict:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        rec = json.load(f)\n",
    "    rec[\"id\"] = json_path.stem\n",
    "    return rec\n",
    "\n",
    "records = [load_example(p) for p in MISCLASS_DIR.glob(\"*.json\")]\n",
    "\n",
    "false_pos, false_neg = [], []\n",
    "for rec in records:\n",
    "    gt, pred = rec[\"ground_truth\"], rec[\"prediction\"]\n",
    "    if gt == \"Excluded\" and pred == \"Included\":\n",
    "        false_pos.append(rec[\"id\"])\n",
    "    elif gt == \"Included\" and pred == \"Excluded\":\n",
    "        false_neg.append(rec[\"id\"])\n",
    "\n",
    "# Keep up to 10 of each, deterministic for reproducibility\n",
    "random.shuffle(false_pos)\n",
    "random.shuffle(false_neg)\n",
    "false_pos = false_pos#[:10]\n",
    "false_neg = false_neg#[:10]\n",
    "\n",
    "def fetch_rows(id_list: List[str]) -> List[Dict]:\n",
    "    subset = train_df[train_df[\"id\"].isin(id_list)]\n",
    "    out = []\n",
    "    for _, r in subset.iterrows():\n",
    "        out.append({\n",
    "            \"id\"      : r[\"id\"],\n",
    "            \"title\"   : r[\"title\"],\n",
    "            \"abstract\": r[\"abstract\"],\n",
    "            \"label\"   : r[\"label\"]\n",
    "        })\n",
    "    return out\n",
    "\n",
    "fp_rows = fetch_rows(false_pos)\n",
    "fn_rows = fetch_rows(false_neg)\n",
    "\n",
    "print(f\"Selected {len(fp_rows)} false-positives and {len(fn_rows)} false-negatives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 3 – Build meta-prompt for the LLM ✍️      ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "orig_prompt = PROMPT_V1_PATH.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def format_example(ex: Dict) -> str:\n",
    "    return (\n",
    "        f\"\\n---\\nID: {ex['id']}\\nTRUE LABEL: {ex['label']}\\n\"\n",
    "        f\"TITLE: {ex['title']}\\nABSTRACT: {ex['abstract']}\\n\"\n",
    "    )\n",
    "\n",
    "examples_text  = \"\\nFALSE POSITIVES:\" + \"\".join(format_example(e) for e in fp_rows)\n",
    "examples_text += \"\\n\\nFALSE NEGATIVES:\" + \"\".join(format_example(e) for e in fn_rows)\n",
    "\n",
    "meta_prompt = f\"\"\"\n",
    "You are an expert prompt-engineer.\n",
    "\n",
    "TASK:\n",
    "Rewrite the ORIGINAL PROMPT so that an LLM classifies abstracts as **Included** or **Excluded** with higher accuracy.\n",
    "Return ONLY the improved prompt text, nothing else.\n",
    "\n",
    "CONSTRAINTS:\n",
    "• Minimize your changes to the original prompt.\n",
    "• The changes you make should be based on the misclassified examples provided.\n",
    "• All changes should be small and incremental, not large rewrites.\n",
    "• Keep the JSON schema unchanged.\n",
    "• Use clear, concise instructions.\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "{orig_prompt}\n",
    "Below are misclassified examples from the training split to help you refine the instructions:\n",
    "{examples_text}\n",
    "\n",
    "Respond with the new prompt only.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b742880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 4 - Call Azure OpenAI for new prompt 🤖   ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "def get_client() -> AzureOpenAI:\n",
    "    return AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"ENDPOINT_URL\"),\n",
    "        api_version=AZ_VERSION,\n",
    "    )\n",
    "\n",
    "client = get_client()\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(5))\n",
    "def call_llm(prompt: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # Try a series of argument sets, falling back when the model rejects a parameter\n",
    "    attempts = [\n",
    "        {\"max_completion_tokens\": 4000, \"temperature\": 0.7, \"top_p\": 1},\n",
    "        {\"max_tokens\": 4000, \"temperature\": 0.7, \"top_p\": 1},\n",
    "        {\"max_completion_tokens\": 4000},\n",
    "        {\"max_tokens\": 4000},\n",
    "    ]\n",
    "\n",
    "    last_err = None\n",
    "    for kwargs in attempts:\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=DEPLOYMENT_NAME,\n",
    "                messages=messages,\n",
    "                **kwargs,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            # Save error and keep looping if it looks like a parameter issue\n",
    "            msg = str(e)\n",
    "            if (\n",
    "                \"unsupported_parameter\" in msg\n",
    "                or \"unsupported_value\" in msg\n",
    "                or \"does not support\" in msg\n",
    "            ):\n",
    "                last_err = e\n",
    "                continue\n",
    "            raise  # re-raise unexpected errors\n",
    "    # If we exit the loop, re-raise the last captured error\n",
    "    raise last_err  # type: ignore[misc]\n",
    "\n",
    "\n",
    "new_prompt_text = call_llm(meta_prompt)\n",
    "print(\"LLM returned a prompt with\", len(new_prompt_text.split()), \"words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98412970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 5 – Save new prompt 💾                    ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "PROMPT_V2_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "PROMPT_V2_PATH.write_text(new_prompt_text, encoding=\"utf-8\")\n",
    "print(\"Saved improved prompt to:\", PROMPT_V2_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa8be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
