{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560e0eb",
   "metadata": {},
   "source": [
    "# ğŸ“ŠğŸ§ª Literature Screening â€“ Model Evaluation  \n",
    "Aggregate results for **train** and **test** splits, report metrics and confusion matrices separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 1 â€“ Imports and helpers ğŸ”Œ               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 2 â€“ Discover model folders ğŸ”            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "outputs_root = Path(\"outputs\")\n",
    "model_dirs = [d for d in outputs_root.iterdir() if d.is_dir() and d.name != \"datasets\"]\n",
    "\n",
    "if not model_dirs:\n",
    "    raise RuntimeError(\"No model result folders found inside 'outputs/'\")\n",
    "\n",
    "print(\"Models found:\", \", \".join(d.name for d in model_dirs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 3 â€“ Load predictions per split ğŸ—„ï¸        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "all_predictions = defaultdict(dict)  # {model: {split: DataFrame}}\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = mdir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for jf in preds_dir.glob(\"*.json\"):\n",
    "            with open(jf, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            rows.append({\n",
    "                \"id\"          : jf.stem,\n",
    "                \"ground_truth\": data.get(\"ground_truth\"),\n",
    "                \"prediction\"  : data.get(\"prediction\")\n",
    "            })\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            all_predictions[mdir.name][split] = df\n",
    "            unparsable = (df[\"prediction\"] == \"ParseError\").sum()\n",
    "            print(f\"{mdir.name} [{split}] -> {len(df):,} rows, {unparsable} unparsable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ebbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 4 â€“ Build metrics table ğŸ“‹               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "metrics = []\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"prediction\"].isin([\"Included\", \"Excluded\"])]\n",
    "        unparsed = len(df) - len(parsable)\n",
    "\n",
    "        if len(parsable) == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"prediction\"]\n",
    "\n",
    "        metrics.append({\n",
    "            \"model\"      : model,\n",
    "            \"split\"      : split,\n",
    "            \"n_total\"    : len(df),\n",
    "            \"n_unparsed\" : unparsed,\n",
    "            \"accuracy\"   : accuracy_score(y_true, y_pred),\n",
    "            \"precision\"  : precision_score(y_true, y_pred, pos_label=\"Included\"),\n",
    "            \"recall\"     : recall_score(y_true, y_pred,  pos_label=\"Included\"),\n",
    "            \"f1\"         : f1_score(y_true, y_pred,      pos_label=\"Included\")\n",
    "        })\n",
    "\n",
    "metrics_df = (pd.DataFrame(metrics)\n",
    "              .set_index([\"model\", \"split\"])\n",
    "              .sort_values([\"model\", \"split\"]))\n",
    "\n",
    "metrics_df.style.format({\n",
    "    \"accuracy\" : \"{:.3f}\",\n",
    "    \"precision\": \"{:.3f}\",\n",
    "    \"recall\"   : \"{:.3f}\",\n",
    "    \"f1\"       : \"{:.3f}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 5 â€“ Bar charts for each split ğŸ“Š         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "for split in (\"train\", \"test\"):\n",
    "    subset = metrics_df.xs(split, level=\"split\")\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    subset[\"accuracy\"].plot(kind=\"bar\", ax=axes[0], color=\"mediumseagreen\")\n",
    "    axes[0].set_title(f\"Accuracy ({split})\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    subset[\"f1\"].plot(kind=\"bar\", ax=axes[1], color=\"dodgerblue\")\n",
    "    axes[1].set_title(f\"F1-score ({split})\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    (subset[\"n_unparsed\"] / subset[\"n_total\"]).plot(kind=\"bar\",\n",
    "                                                    ax=axes[2],\n",
    "                                                    color=\"indianred\")\n",
    "    axes[2].set_title(f\"Unparsed % ({split})\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(f\"Model comparison on {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Cell 6 â€“ Confusion matrices ğŸ”²                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"prediction\"].isin([\"Included\", \"Excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"prediction\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"Included\", \"Excluded\"])\n",
    "\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm,\n",
    "                    annot=True,\n",
    "                    fmt=\"d\",\n",
    "                    cmap=\"Purples\",\n",
    "                    xticklabels=[\"Included\", \"Excluded\"],\n",
    "                    yticklabels=[\"Included\", \"Excluded\"])\n",
    "        plt.title(f\"Confusion Matrix - {model} ({split})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Classification report for {model} ({split})\")\n",
    "        print(classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decc02b",
   "metadata": {},
   "source": [
    "## âœ”ï¸ Evaluation complete  \n",
    "You now get clear, separate insights for train and test splits across every model folder found under **outputs/**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea716356",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
