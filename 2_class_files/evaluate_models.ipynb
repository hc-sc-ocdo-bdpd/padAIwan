{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7560e0eb",
   "metadata": {},
   "source": [
    "# 📊🧪 Literature Screening – Model Evaluation  \n",
    "Aggregate results for **train** and **test** splits, report metrics and confusion matrices separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 1 – Imports and helpers 🔌               ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 2 – Discover model folders 🔍            ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "outputs_root = Path(\"outputs\")\n",
    "model_dirs = [d for d in outputs_root.iterdir() if d.is_dir() and d.name != \"datasets\"]\n",
    "\n",
    "if not model_dirs:\n",
    "    raise RuntimeError(\"No model result folders found inside 'outputs/'\")\n",
    "\n",
    "print(\"Models found:\", \", \".join(d.name for d in model_dirs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 3 – Load predictions per split 🗄️        ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "all_predictions = defaultdict(dict)  # {model: {split: DataFrame}}\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    for split in (\"train\", \"test\"):\n",
    "        preds_dir = mdir / split / \"predictions\"\n",
    "        if not preds_dir.exists():\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for jf in preds_dir.glob(\"*.json\"):\n",
    "            with open(jf, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            rows.append({\n",
    "                \"id\"          : jf.stem,\n",
    "                \"ground_truth\": data.get(\"ground_truth\"),\n",
    "                \"prediction\"  : data.get(\"prediction\")\n",
    "            })\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            all_predictions[mdir.name][split] = df\n",
    "            unparsable = (df[\"prediction\"] == \"ParseError\").sum()\n",
    "            print(f\"{mdir.name} [{split}] -> {len(df):,} rows, {unparsable} unparsable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ebbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 4 – Build metrics table 📋               ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "metrics = []\n",
    "\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"prediction\"].isin([\"Included\", \"Excluded\"])]\n",
    "        unparsed = len(df) - len(parsable)\n",
    "\n",
    "        if len(parsable) == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"prediction\"]\n",
    "\n",
    "        metrics.append({\n",
    "            \"model\"      : model,\n",
    "            \"split\"      : split,\n",
    "            \"n_total\"    : len(df),\n",
    "            \"n_unparsed\" : unparsed,\n",
    "            \"accuracy\"   : accuracy_score(y_true, y_pred),\n",
    "            \"precision\"  : precision_score(y_true, y_pred, pos_label=\"Included\"),\n",
    "            \"recall\"     : recall_score(y_true, y_pred,  pos_label=\"Included\"),\n",
    "            \"f1\"         : f1_score(y_true, y_pred,      pos_label=\"Included\")\n",
    "        })\n",
    "\n",
    "metrics_df = (pd.DataFrame(metrics)\n",
    "              .set_index([\"model\", \"split\"])\n",
    "              .sort_values([\"model\", \"split\"]))\n",
    "\n",
    "metrics_df.style.format({\n",
    "    \"accuracy\" : \"{:.3f}\",\n",
    "    \"precision\": \"{:.3f}\",\n",
    "    \"recall\"   : \"{:.3f}\",\n",
    "    \"f1\"       : \"{:.3f}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 5 – Bar charts for each split 📊         ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for split in (\"train\", \"test\"):\n",
    "    subset = metrics_df.xs(split, level=\"split\")\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    subset[\"accuracy\"].plot(kind=\"bar\", ax=axes[0], color=\"mediumseagreen\")\n",
    "    axes[0].set_title(f\"Accuracy ({split})\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    subset[\"f1\"].plot(kind=\"bar\", ax=axes[1], color=\"dodgerblue\")\n",
    "    axes[1].set_title(f\"F1-score ({split})\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    (subset[\"n_unparsed\"] / subset[\"n_total\"]).plot(kind=\"bar\",\n",
    "                                                    ax=axes[2],\n",
    "                                                    color=\"indianred\")\n",
    "    axes[2].set_title(f\"Unparsed % ({split})\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.suptitle(f\"Model comparison on {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║ Cell 6 – Confusion matrices 🔲                ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "for model, split_dict in all_predictions.items():\n",
    "    for split, df in split_dict.items():\n",
    "        parsable = df[df[\"prediction\"].isin([\"Included\", \"Excluded\"])]\n",
    "        if parsable.empty:\n",
    "            continue\n",
    "\n",
    "        y_true = parsable[\"ground_truth\"]\n",
    "        y_pred = parsable[\"prediction\"]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"Included\", \"Excluded\"])\n",
    "\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm,\n",
    "                    annot=True,\n",
    "                    fmt=\"d\",\n",
    "                    cmap=\"Purples\",\n",
    "                    xticklabels=[\"Included\", \"Excluded\"],\n",
    "                    yticklabels=[\"Included\", \"Excluded\"])\n",
    "        plt.title(f\"Confusion Matrix - {model} ({split})\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Classification report for {model} ({split})\")\n",
    "        print(classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decc02b",
   "metadata": {},
   "source": [
    "## ✔️ Evaluation complete  \n",
    "You now get clear, separate insights for train and test splits across every model folder found under **outputs/**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea716356",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
